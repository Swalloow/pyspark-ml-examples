{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark ML GBTClassifier + Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Spark ML\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipelines\n",
    "\n",
    "- http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml\n",
    "- 복잡한 ML 과정들을 파이프라인으로 모듈화 시킬 수 있도록 도와주는 패키지\n",
    "\n",
    "#### Transformer\n",
    "\n",
    "- DataFrame을 **lazily** 하게 또 다른 DataFrame으로 변형, `transform()` 메서드 구현\n",
    "- Feature Engineering에 필요한 알고리즘들뿐만 아니라, 이미 학습이 끝난 Model도 이에 해당\n",
    "\n",
    "---\n",
    "#### Estimator\n",
    "\n",
    "- DataFrame을 model에 fitting 시키는 단계, 학습시키는 알고리즘이 모두 이에 해당\n",
    "- 예를 들면 `LogisticRegression`은 `Estimator`에 해당\n",
    "- `fit()` 함수를 호출하여 생성된 `LogisticRegressionModel`은 `Model`이자 `Transformer`\n",
    "\n",
    "---\n",
    "#### Pipeline\n",
    "\n",
    "- ML을 돌리기 위해 필요한 stage를 연결시킨 구현체\n",
    "- `Transformer`, `Estimator`가 Pipeline의 각 stage에 해당\n",
    "- `Pipeline.fit()`을 호출하면 각 단계에서 지정한 함수를 순서대로 호출\n",
    "- 마찬가지로 `PipelineModel`은 `fit()` 함수를 호출하여 생성된 `Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler, Imputer, StringIndexer\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold value\n",
    "fold = 3\n",
    "\n",
    "# Read train, test dataset\n",
    "inputCols = ['Pclass', 'age_im', 'SibSp', 'Parch', 'Fare', 'embarked_ix', 'sex_ix', 'len_name']\n",
    "str_length = udf(lambda x: len(x), IntegerType())\n",
    "\n",
    "train = spark.read \\\n",
    "    .csv(\"../dataset/train.csv\", header=True, inferSchema=True) \\\n",
    "    .withColumnRenamed(\"Survived\", \"label\") \\\n",
    "    .withColumn('len_name', str_length(col('name'))) \\\n",
    "    .na.drop(subset=[\"Embarked\", \"Fare\"]) \\\n",
    "    .cache()\n",
    "\n",
    "test = spark.read \\\n",
    "    .csv(\"../dataset/test.csv\", header=True, inferSchema=True) \\\n",
    "    .withColumnRenamed(\"Survived\", \"label\") \\\n",
    "    .withColumn('len_name', str_length(col('name'))) \\\n",
    "    .na.drop(subset=[\"Embarked\", \"Fare\"]) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make pipeline, model fitting...\n",
      "Model training finished!\n",
      "Cross-validation average score : 0.8117291437383991\n",
      "Best maxDepth parameters : 5\n",
      "Best maxIter parameters : 40\n",
      "Best maxBins parameters : 25\n"
     ]
    }
   ],
   "source": [
    "# Define operators\n",
    "imputer = Imputer(inputCols=['Age'], outputCols=['age_im'], strategy='mean')\n",
    "sex_ix = StringIndexer(inputCol='Sex', outputCol='sex_ix')\n",
    "embarked_ix = StringIndexer(inputCol='Embarked', outputCol='embarked_ix')\n",
    "assembler = VectorAssembler(inputCols=inputCols, outputCol='features')\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withStd=True, withMean=False)\n",
    "model = GBTClassifier(labelCol='label', featuresCol='scaled_features', cacheNodeIds=True)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "\n",
    "# Pipeline\n",
    "print(\"Make pipeline, model fitting...\")\n",
    "pipeline = Pipeline(\n",
    "    stages=[imputer, sex_ix, embarked_ix, assembler, scaler, model])\n",
    "\n",
    "# K-Fold Cross-validation with Parameter tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(model.maxDepth, [5, 7]) \\\n",
    "    .addGrid(model.maxIter, [20, 40]) \\\n",
    "    .addGrid(model.maxBins, [25]) \\\n",
    "    .addGrid(model.stepSize, [0.025]) \\\n",
    "    .addGrid(model.subsamplingRate, [0.7]) \\\n",
    "    .build()\n",
    "\n",
    "# Fold 3 * Param 4 = 12\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    evaluator=evaluator,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    numFolds=fold)\n",
    "\n",
    "# Model training\n",
    "cvModel = cv.fit(train)\n",
    "bestModel = cvModel.bestModel\n",
    "\n",
    "print(\"Model training finished!\")\n",
    "print(\"Cross-validation average score : {}\".format(cvModel.avgMetrics[0]))\n",
    "print(\"Best maxDepth parameters : {}\".format(bestModel.stages[5]._java_obj.getMaxDepth()))\n",
    "print(\"Best maxIter parameters : {}\".format(bestModel.stages[5]._java_obj.getMaxIter()))\n",
    "print(\"Best maxBins parameters : {}\".format(bestModel.stages[5]._java_obj.getMaxBins()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to csv finished!\n"
     ]
    }
   ],
   "source": [
    "# Save prediction result\n",
    "predict = bestModel.transform(test)\n",
    "predict.select(\"PassengerId\", \"prediction\") \\\n",
    "    .coalesce(1).write.mode(\"overwrite\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .csv(\"../dataset/pred_titanic.csv.gzip\", sep=\",\", header=True)\n",
    "\n",
    "print(\"Save to csv finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance:\n",
      "Pclass = 0.0635219203849\n",
      "age_im = 0.196579680469\n",
      "SibSp = 0.0553845897311\n",
      "Parch = 0.0267508377674\n",
      "Fare = 0.240002914139\n",
      "embarked_ix = 0.064995764243\n",
      "sex_ix = 0.088147713686\n",
      "len_name = 0.264616579579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "featureImportance = bestModel.stages[-1].featureImportances.toArray()\n",
    "print(\"Feature importance:\\n{}\\n\".format(\n",
    "    \"\\n\".join(map(lambda x: \"{} = {}\".format(str(x[0]), str(x[1])), zip(inputCols, featureImportance)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
