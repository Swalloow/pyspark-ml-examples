{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark ML Tutorial\n",
    "\n",
    "- 주의: RDD 기반의 MLlib이 아닌 DataFrame 기반의 ML 패키지를 설명할 예정\n",
    "- 점수 올리는게 목적이 아닌 Spark ML을 활용하는 방법에 대한 설명\n",
    "- 사용한 버전: spark 2.2+, python 3.5+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Spark ML\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset - Kaggle Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"../data/train.csv\") \\\n",
    "    .cache()\n",
    "\n",
    "df.createOrReplaceTempView(\"train\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## EDA: Spark SQL + Zeppelin\n",
    "\n",
    "- EDA는 분산쿼리를 통해 빠르게 수행\n",
    "- `Zeppelin` 환경을 구축해서 쿼리에 대한 그래프를 바로 확인 가능\n",
    "- `printSchema()`, `describe()`, `isNull()`, `select()` 함수를 통해 데이터 상태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|               714|\n",
      "|   mean| 29.69911764705882|\n",
      "| stddev|14.526497332334035|\n",
      "|    min|              0.42|\n",
      "|    max|              80.0|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(['age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# column들에 대한 null 체크\n",
    "df.select(*(\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "    for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Embarked|count|\n",
      "+--------+-----+\n",
      "|       Q|   30|\n",
      "|    null|    2|\n",
      "|       C|   93|\n",
      "|       S|  217|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT Embarked, count(PassengerId) as count\n",
    "FROM train\n",
    "WHERE Survived = 1\n",
    "GROUP BY Embarked\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|PClass|count|\n",
      "+------+-----+\n",
      "|     1|  136|\n",
      "|     3|  119|\n",
      "|     2|   87|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT PClass, count(PassengerId) as count\n",
    "FROM train\n",
    "WHERE Survived = 1\n",
    "GROUP BY PClass\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preprocessing: Spark DataFrame Function + UDF\n",
    "\n",
    "#### Missing Value\n",
    "\n",
    "- `pyspark.sql.DataFrameNaFunctions`에서 확인\n",
    "- Spark ML의 `Imputer`로도 처리 가능 (Pipeline과의 연계)\n",
    "- http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions\n",
    "\n",
    "---\n",
    "#### Feature Engineering\n",
    "\n",
    "- udf를 만들어서 내가 원하는 형태로 전처리 가능\n",
    "- approxQuantile, correlation, covariance, stratified sampling 등이 필요한 경우\n",
    "- `pyspark.sql.DataFrameStatFunctions`에서 확인\n",
    "- http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameStatFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 177\n",
      "After 0\n"
     ]
    }
   ],
   "source": [
    "# column에서 null 값을 drop 시키는 경우\n",
    "df = df.drop('cabin')\n",
    "before = df.select('age').where('age is null').count()\n",
    "print(\"Before: {}\".format(before))\n",
    "\n",
    "test = df.na.drop(subset=[\"age\"])\n",
    "after = test.select('age').where('age is null').count()\n",
    "print(\"After {}\".format(after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|22.0|\n",
      "|38.0|\n",
      "|26.0|\n",
      "|35.0|\n",
      "|35.0|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# column에서 null 값을 mean으로 채우는 경우\n",
    "avg_age = df.where('age is not null').groupBy().avg('age').collect()[0][0]\n",
    "df = df.na.fill({'age': avg_age})\n",
    "df.select('age').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:\n",
      "+--------+-----+\n",
      "|survived|count|\n",
      "+--------+-----+\n",
      "|       1|  342|\n",
      "|       0|  549|\n",
      "+--------+-----+\n",
      "\n",
      "After:\n",
      "+--------+-----+\n",
      "|survived|count|\n",
      "+--------+-----+\n",
      "|       1|  168|\n",
      "|       0|   57|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# label을 기준으로 Stratified Sampling 예시\n",
    "sample_df = df.sampleBy('survived', fractions={0: 0.1, 1: 0.5}, seed=0)\n",
    "print(\"Before:\")\n",
    "df.groupBy('survived').count().show()\n",
    "print(\"After:\")\n",
    "sample_df.groupBy('survived').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                name|len_name|\n",
      "+--------------------+--------+\n",
      "|Braund, Mr. Owen ...|      23|\n",
      "|Cumings, Mrs. Joh...|      51|\n",
      "|Heikkinen, Miss. ...|      22|\n",
      "|Futrelle, Mrs. Ja...|      44|\n",
      "|Allen, Mr. Willia...|      24|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 승객 이름의 길이를 새로운 feature로 추가하는 예시\n",
    "str_length = udf(lambda x: len(x), IntegerType())\n",
    "df = df.withColumn('len_name', str_length(df['name']))\n",
    "df.select('name', 'len_name').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|embarked|embarked_ix|\n",
      "+--------+-----------+\n",
      "|       S|          3|\n",
      "|       C|          1|\n",
      "|       S|          3|\n",
      "|       S|          3|\n",
      "|       S|          3|\n",
      "+--------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# udf를 사용해서 categorical feature를 전처리하는 예시\n",
    "# Spark ML의 StringIndexer를 사용해도 결과는 동일\n",
    "\n",
    "def embarked_to_int(embarked):\n",
    "    if embarked == 'C': return 1\n",
    "    elif embarked == 'Q': return 2\n",
    "    elif embarked == 'S': return 3    \n",
    "    else: return 0\n",
    "\n",
    "embarked_to_int = udf(embarked_to_int, IntegerType())\n",
    "df = df.withColumn('embarked_ix', embarked_to_int(df['embarked']))\n",
    "df.select('embarked', 'embarked_ix').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|   sex|sex_ix|\n",
      "+------+------+\n",
      "|  male|     0|\n",
      "|female|     1|\n",
      "|female|     1|\n",
      "|female|     1|\n",
      "|  male|     0|\n",
      "+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL Function의 when-otherwise 절을 사용하는 방법\n",
    "# categorical feature를 전처리하는 예시\n",
    "df.select('sex', \n",
    "    when(df['sex'] == 'male', 0).otherwise(1).alias('sex_ix')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Extracting, transforming and selecting features\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.feature\n",
    "\n",
    "#### Extraction\n",
    "\n",
    "- raw 데이터로부터 feature를 추출하기 위한 패키지\n",
    "- `TF-IDF`, `Word2Vec`, `CountVectorizer`, `FeatureHasher`\n",
    "\n",
    "---\n",
    "#### Transformation\n",
    "\n",
    "- feature를 변형시키기 위한 패키지 (scaling, coverting)\n",
    "- `Tokenizer`, `StopWordsRemover`, `n-gram`, `PCA`, `StringIndexer`, `OneHotEncoder`\n",
    "- `StandardScaler`, `MinMaxScaler` 등\n",
    "\n",
    "---\n",
    "#### Selection\n",
    "\n",
    "- feature selection을 지원하는 패키지 (feature가 정말 많은 경우 유용)\n",
    "- `VectorSlicer`, `RFormula`, `ChiSqSelector`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|   Sex|sex_ix|\n",
      "+------+------+\n",
      "|  male|   0.0|\n",
      "|female|   1.0|\n",
      "|female|   1.0|\n",
      "|female|   1.0|\n",
      "|  male|   0.0|\n",
      "+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# StringIndexer를 사용해서 categorical feature를 전처리하는 예시\n",
    "df = StringIndexer(inputCol='Sex', outputCol='sex_ix').fit(df).transform(df)\n",
    "df.select('Sex', 'sex_ix').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------------------+\n",
      "|PassengerId|label|            features|\n",
      "+-----------+-----+--------------------+\n",
      "|          1|    0|[3.0,22.0,1.0,0.0...|\n",
      "|          2|    1|[1.0,38.0,1.0,0.0...|\n",
      "|          3|    1|[3.0,26.0,0.0,0.0...|\n",
      "|          4|    1|[1.0,35.0,1.0,0.0...|\n",
      "|          5|    0|[3.0,35.0,0.0,0.0...|\n",
      "+-----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VectorAssembler를 사용해서 feature를 vector 형태로 변환\n",
    "inputCols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'embarked_ix', 'sex_ix', 'len_name']\n",
    "assembler = VectorAssembler(inputCols=inputCols, outputCol='features')\n",
    "train = assembler.transform(df).select('PassengerId', col('Survived').alias('label'), 'features')\n",
    "train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model\n",
    "- http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.classification\n",
    "- 대부분 **Data parallelism**을 통해 분산학습하는 방식\n",
    "- Spark 2.3 버전부터 **Model parallelism**을 지원\n",
    "\n",
    "#### Classification, Regression\n",
    "\n",
    "- 트리 모델: `DecisionTree`, `RandomForest`, `GBTClassifier`\n",
    "- SVM 모델: `LinearSVC`, `OneVsRest`\n",
    "- `MultilayerPerceptronClassifier`: hidden layer가 없는 Softmax 모델\n",
    "- `LinearRegression`, `SurvivalRegression`, `NaiveBayes`\n",
    "\n",
    "---\n",
    "#### Clustering\n",
    "\n",
    "- 다양한 클러스터링 알고리즘을 지원\n",
    "- `KMeans`, `LDA`, `GMM`\n",
    "- 이전에는 computeCost 함수를 통해 SSE로 모델을 평가\n",
    "- 2.3 버전부터 `ClusteringEvaluator` 사용 가능\n",
    "\n",
    "---\n",
    "#### Recommendation\n",
    "\n",
    "- CF 방식의 `Alternating Least Squares(ALS)` 추천 알고리즘을 지원\n",
    "- \"Large-Scale Parallel Collaborative Filtering for the Netflix Prize\" 논문을 참고\n",
    "- Production에 쉽게 연동할 수 있게 만든 **Apache PredictionIO**도 참고 (MLlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|PassengerId|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|         27|    0|[3.0,29.699117647...|[16.4071621231905...|[0.82035810615952...|       0.0|\n",
      "|         34|    0|[2.0,66.0,0.0,0.0...|[17.1275400788451...|[0.85637700394225...|       0.0|\n",
      "|         44|    1|[2.0,3.0,1.0,2.0,...|[0.35197754315401...|[0.01759887715770...|       1.0|\n",
      "|         49|    0|[3.0,29.699117647...|[14.2343850263881...|[0.71171925131940...|       0.0|\n",
      "|         50|    0|[3.0,18.0,1.0,0.0...|[7.40981147634526...|[0.37049057381726...|       1.0|\n",
      "+-----------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RandomForestClassifier 예제\n",
    "# training set을 row 단위로 partitioning\n",
    "splits = train.randomSplit([0.8, 0.2])\n",
    "train = splits[0].cache()\n",
    "test = splits[1].cache()\n",
    "\n",
    "# cacheNodeIds: 인스턴스 마다 노드의 Id를 캐싱, 트리가 깊어진다면 성능 향상 팁\n",
    "model = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    cacheNodeIds=True)\n",
    "\n",
    "predict = model.fit(train).transform(test)\n",
    "predict.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.evaluation\n",
    "\n",
    "- 모델을 평가하기 위한 패키지, 사용할 수 있는 metric을 확인할 필요가 있음\n",
    "- BinaryClassificationEvaluator: `areaUnderROC`만 사용 가능\n",
    "- MulticlassClassificationEvaluator: `f1`, `weightedPrecision`, `weightedRecall`, `accuracy`\n",
    "- RegressionEvaluator: `rmse`, `mse`, `mae`\n",
    "- ClusteringEvaluator: 2.3 버전에 새롭게 추가, metric으로 `silhouette` 사용 가능\n",
    "- `confusionMatrix()` 등 몇 가지는 아직 Spark MLlib에만 존재함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8469387755102041"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    predictionCol=\"prediction\", \n",
    "    labelCol=\"label\", \n",
    "    metricName=\"accuracy\")\n",
    "\n",
    "evaluator.evaluate(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Tuning: model selection and hyperparameter tuning\n",
    "- http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml.tuning\n",
    "- 지정한 parameter의 조합에 대하여 반복 학습하는 형태\n",
    "- 원래 `data parallelism` 만 지원했지만, 2.3버전부터 `model parallelism`도 지원하기 시작\n",
    "- CrossValidator와 TrainValidationSplit에 `parallelism` 파라메터 지정\n",
    "\n",
    "#### ParamGridBuilder\n",
    "\n",
    "- 파라메터를 자동으로 튜닝하기 위한 빌더 패키지 (Grid Search)\n",
    "- 각 모델에 대한 파라메터는 `spark.ml.param` module\n",
    "\n",
    "---\n",
    "#### CrossValidator\n",
    "\n",
    "- K-Fold CrossValidation 그 자체 (위키 참고)\n",
    "- 지정한 Fold 만큼 반복 학습\n",
    "\n",
    "---\n",
    "#### TrainValidationSplit (Experimental)\n",
    "\n",
    "- 지정한 비율에 따라 훈련/검증 셋을 나누어 학습에 반영\n",
    "- CrossValidator에 비해 금방 끝나겠지만, 주어진 학습 데이터가 적다면 결과가 부정확할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8622448979591837"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modeling\n",
    "model = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    cacheNodeIds=True)\n",
    "\n",
    "# Parameter tuning\n",
    "# 2 * 2 = 4번 학습 진행\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(model.numTrees, [500, 700]) \\\n",
    "    .addGrid(model.maxDepth, [5, 7]) \\\n",
    "    .addGrid(model.impurity, [\"gini\"]) \\\n",
    "    .addGrid(model.maxBins, [31]) \\\n",
    "    .addGrid(model.subsamplingRate, [0.7]) \\\n",
    "    .build()\n",
    "\n",
    "# Evaluator: accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    predictionCol=\"prediction\", \n",
    "    labelCol=\"label\", \n",
    "    metricName=\"accuracy\")\n",
    "\n",
    "# train:validation = 7:3\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=model,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio=0.7)\n",
    "\n",
    "tvsModel = tvs.fit(train)\n",
    "predict = tvsModel.transform(test)\n",
    "evaluator.evaluate(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[PassengerId: int, Survived: int, Pclass: int, Name: string, Sex: string, Age: double, SibSp: int, Parch: int, Ticket: string, Fare: double, Embarked: string, len_name: int, embarked_ix: int, sex_ix: double]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.unpersist()\n",
    "test.unpersist()\n",
    "df.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Pipelines\n",
    "\n",
    "- http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#module-pyspark.ml\n",
    "- 복잡한 ML 과정들을 파이프라인으로 모듈화 시킬 수 있도록 도와주는 패키지\n",
    "\n",
    "#### Transformer\n",
    "\n",
    "- DataFrame을 **lazily** 하게 또 다른 DataFrame으로 변형, `transform()` 메서드 구현\n",
    "- Feature Engineering에 필요한 알고리즘들뿐만 아니라, 이미 학습이 끝난 Model도 이에 해당\n",
    "\n",
    "---\n",
    "#### Estimator\n",
    "\n",
    "- DataFrame을 model에 fitting 시키는 단계, 학습시키는 알고리즘이 모두 이에 해당\n",
    "- 예를 들면 `LogisticRegression`은 `Estimator`에 해당\n",
    "- `fit()` 함수를 호출하여 생성된 `LogisticRegressionModel`은 `Model`이자 `Transformer`\n",
    "\n",
    "---\n",
    "#### Pipeline\n",
    "\n",
    "- ML을 돌리기 위해 필요한 stage를 연결시킨 구현체\n",
    "- `Transformer`, `Estimator`가 Pipeline의 각 stage에 해당\n",
    "- `Pipeline.fit()`을 호출하면 각 단계에서 지정한 함수를 순서대로 호출\n",
    "- 마찬가지로 `PipelineModel`은 `fit()` 함수를 호출하여 생성된 `Model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make pipeline, model fitting...\n",
      "Model training finished!\n",
      "Cross-validation average score : 0.8054909474865615\n",
      "Best maxDepth parameters : 5\n",
      "Best maxIter parameters : 40\n",
      "Best maxBins parameters : 25\n"
     ]
    }
   ],
   "source": [
    "# K-Fold value\n",
    "fold = 3\n",
    "\n",
    "# Read train, test dataset\n",
    "inputCols = ['Pclass', 'age_im', 'SibSp', 'Parch', 'Fare', 'embarked_ix', 'sex_ix', 'len_name']\n",
    "str_length = udf(lambda x: len(x), IntegerType())\n",
    "\n",
    "train = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"../data/train.csv\") \\\n",
    "    .withColumnRenamed(\"Survived\", \"label\") \\\n",
    "    .withColumn('len_name', str_length(col('name'))) \\\n",
    "    .na.drop(subset=[\"Embarked\", \"Fare\"]) \\\n",
    "    .cache()\n",
    "\n",
    "test = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"../data/test.csv\") \\\n",
    "    .withColumnRenamed(\"Survived\", \"label\") \\\n",
    "    .withColumn('len_name', str_length(col('name'))) \\\n",
    "    .na.drop(subset=[\"Embarked\", \"Fare\"]) \\\n",
    "    .cache()\n",
    "\n",
    "# Define operators\n",
    "imputer = Imputer(inputCols=['Age'], outputCols=['age_im'], strategy='mean')\n",
    "sex_ix = StringIndexer(inputCol='Sex', outputCol='sex_ix')\n",
    "embarked_ix = StringIndexer(inputCol='Embarked', outputCol='embarked_ix')\n",
    "assembler = VectorAssembler(inputCols=inputCols, outputCol='features')\n",
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withStd=True, withMean=False)\n",
    "model = GBTClassifier(labelCol='label', featuresCol='scaled_features', cacheNodeIds=True)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "\n",
    "# Pipeline\n",
    "print(\"Make pipeline, model fitting...\")\n",
    "pipeline = Pipeline(\n",
    "    stages=[imputer, sex_ix, embarked_ix, assembler, scaler, model])\n",
    "\n",
    "# K-Fold Cross-validation with Parameter tuning\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(model.maxDepth, [5, 7]) \\\n",
    "    .addGrid(model.maxIter, [20, 40]) \\\n",
    "    .addGrid(model.maxBins, [25]) \\\n",
    "    .addGrid(model.stepSize, [0.025]) \\\n",
    "    .addGrid(model.subsamplingRate, [0.7]) \\\n",
    "    .build()\n",
    "\n",
    "# Fold 3 * Param 4 = 12\n",
    "cv = CrossValidator(estimator=pipeline,\n",
    "                    evaluator=evaluator,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    numFolds=fold)\n",
    "\n",
    "# Model training\n",
    "cvModel = cv.fit(train)\n",
    "bestModel = cvModel.bestModel\n",
    "\n",
    "print(\"Model training finished!\")\n",
    "print(\"Cross-validation average score : {}\".format(cvModel.avgMetrics[0]))\n",
    "print(\"Best maxDepth parameters : {}\".format(bestModel.stages[5]._java_obj.getMaxDepth()))\n",
    "print(\"Best maxIter parameters : {}\".format(bestModel.stages[5]._java_obj.getMaxIter()))\n",
    "print(\"Best maxBins parameters : {}\".format(bestModel.stages[5]._java_obj.getMaxBins()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to csv finished!\n"
     ]
    }
   ],
   "source": [
    "# Save prediction result\n",
    "predict = bestModel.transform(test)\n",
    "predict.select(\"PassengerId\", \"prediction\") \\\n",
    "    .coalesce(1).write.mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .csv(\"../data/pred_titanic.csv.gzip\", sep=\",\")\n",
    "\n",
    "print(\"Save to csv finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance:\n",
      "Pclass = 0.0645650035273\n",
      "age_im = 0.256040506903\n",
      "SibSp = 0.0494381820459\n",
      "Parch = 0.0383266389843\n",
      "Fare = 0.236916837319\n",
      "embarked_ix = 0.0359084807624\n",
      "sex_ix = 0.0990444690379\n",
      "len_name = 0.21975988142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "featureImportance = bestModel.stages[-1].featureImportances.toArray()\n",
    "print(\"Feature importance:\\n{}\\n\".format(\n",
    "    \"\\n\".join(map(lambda x: \"{} = {}\".format(str(x[0]), str(x[1])), zip(inputCols, featureImportance)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
